{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_string = re.compile(r\"(?:(?:[^а-яА-Я]+')|(?:'[^а-яА-Я]+))|(?:[^а-яА-Я']+)\")\n",
    "words_number_leave = 50000\n",
    "stop_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stop_words.txt', 'r', encoding='UTF-8') as f:\n",
    "    stop_words = f.read().splitlines()\n",
    "stop_words.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "words_per_article = [defaultdict(int) for i in range(60000)]\n",
    "all_words = set()\n",
    "themes = []\n",
    "with open(\"news_train.txt\", \"r\", encoding=\"utf-8\") as train_text:\n",
    "    for index, article in enumerate(train_text):\n",
    "        split = article.split('\\t')\n",
    "        theme = split[0]\n",
    "        themes.append(theme)\n",
    "        text = (split[1] + \" \" + split[2]).lower()\n",
    "        words = regex_string.split(text)\n",
    "        for word in words:\n",
    "            if word not in stop_words:\n",
    "                words_per_article[index][word] += 1\n",
    "                all_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'предложит': 1,\n",
       "             'оон': 4,\n",
       "             'способ': 1,\n",
       "             'защиты': 2,\n",
       "             'интернет': 3,\n",
       "             'революций': 2,\n",
       "             'совет': 1,\n",
       "             'безопасности': 3,\n",
       "             'рф': 2,\n",
       "             'совместно': 1,\n",
       "             'мидом': 1,\n",
       "             'разработал': 1,\n",
       "             'проект': 3,\n",
       "             'конвенции': 4,\n",
       "             'призванной': 1,\n",
       "             'оградить': 1,\n",
       "             'государства': 5,\n",
       "             'вмешательства': 2,\n",
       "             'киберпространство': 1,\n",
       "             'извне': 1,\n",
       "             'документа': 3,\n",
       "             'оказался': 1,\n",
       "             'распоряжении': 1,\n",
       "             'газеты': 1,\n",
       "             'коммерсантъ': 1,\n",
       "             'документ': 1,\n",
       "             'обеспечении': 1,\n",
       "             'международной': 1,\n",
       "             'информационной': 1,\n",
       "             'представлен': 1,\n",
       "             'закрытой': 1,\n",
       "             'встрече': 1,\n",
       "             'руководителей': 1,\n",
       "             'спецслужб': 1,\n",
       "             'силовых': 1,\n",
       "             'ведомств': 1,\n",
       "             'стран': 3,\n",
       "             'проходившей': 1,\n",
       "             'сентября': 1,\n",
       "             'екатеринбурге': 1,\n",
       "             'проекте': 2,\n",
       "             'содержатся': 1,\n",
       "             'положения': 1,\n",
       "             'призванные': 1,\n",
       "             'защитить': 1,\n",
       "             'внешнего': 1,\n",
       "             'киберпространстве': 1,\n",
       "             'среди': 1,\n",
       "             'основных': 1,\n",
       "             'угроз': 1,\n",
       "             'перечисленных': 1,\n",
       "             'четвертой': 1,\n",
       "             'статье': 1,\n",
       "             'называются': 1,\n",
       "             'использование': 1,\n",
       "             'информационных': 1,\n",
       "             'технологий': 1,\n",
       "             'враждебных': 1,\n",
       "             'действий': 1,\n",
       "             'актов': 1,\n",
       "             'агрессии': 1,\n",
       "             'массированная': 1,\n",
       "             'психологическая': 1,\n",
       "             'обработка': 1,\n",
       "             'населения': 1,\n",
       "             'дестабилизации': 1,\n",
       "             'общества': 1,\n",
       "             'отмечается': 1,\n",
       "             'вправе': 1,\n",
       "             'устанавливать': 1,\n",
       "             'нормы': 1,\n",
       "             'своем': 1,\n",
       "             'пространстве': 1,\n",
       "             'управлять': 1,\n",
       "             'соответствии': 1,\n",
       "             'национальным': 1,\n",
       "             'законодательством': 1,\n",
       "             'несмотря': 1,\n",
       "             'документе': 1,\n",
       "             'говорится': 1,\n",
       "             'обязанности': 1,\n",
       "             'защищать': 1,\n",
       "             'свободу': 1,\n",
       "             'слова': 1,\n",
       "             'интернете': 1,\n",
       "             'предполагается': 1,\n",
       "             'правительства': 1,\n",
       "             'накладывать': 1,\n",
       "             'ограничения': 1,\n",
       "             'целях': 1,\n",
       "             'национальной': 1,\n",
       "             'общественной': 1,\n",
       "             'собеседники': 1,\n",
       "             'коммерсанта': 1,\n",
       "             'считают': 1,\n",
       "             'конвенция': 1,\n",
       "             'призвана': 1,\n",
       "             'послужить': 1,\n",
       "             'противовесом': 1,\n",
       "             'политике': 1,\n",
       "             'ряда': 1,\n",
       "             'государств': 1,\n",
       "             'первую': 1,\n",
       "             'очередь': 1,\n",
       "             'сша': 2,\n",
       "             'наращивающих': 1,\n",
       "             'киберпотенциал': 1,\n",
       "             'создающих': 1,\n",
       "             'специализированные': 1,\n",
       "             'войска': 1,\n",
       "             'принятие': 2,\n",
       "             'виде': 1,\n",
       "             'стоящей': 1,\n",
       "             'законодательствами': 1,\n",
       "             'отдельных': 1,\n",
       "             'позволит': 1,\n",
       "             'россии': 1,\n",
       "             'юридические': 1,\n",
       "             'гарантии': 1,\n",
       "             'невмешательства': 1,\n",
       "             'внутренние': 1,\n",
       "             'дела': 1,\n",
       "             'отмечают': 1,\n",
       "             'эксперты': 1,\n",
       "             'представители': 1,\n",
       "             'мид': 1,\n",
       "             'сообщили': 1,\n",
       "             'рассчитывают': 1,\n",
       "             'потребуется': 1,\n",
       "             'убедить': 1,\n",
       "             'членов': 1,\n",
       "             'российский': 1,\n",
       "             'преимуществ': 1,\n",
       "             'ответные': 1,\n",
       "             'инициативы': 1,\n",
       "             'некоторых': 1,\n",
       "             'европейских': 1,\n",
       "             'представлены': 1,\n",
       "             'ноябре': 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_per_article[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8388832"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "getsizeof(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, lenWords):\n",
    "    tfDict = defaultdict(float)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/lenWords\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsLen = len(all_words)\n",
    "tf_per_article = [defaultdict(float) for i in range(60000)]\n",
    "for index, wordsDict in enumerate(words_per_article):\n",
    "    tf_per_article[index] = computeTF(wordsDict, float(wordsLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList, all_words, N):\n",
    "    idfDict = dict.fromkeys(all_words, 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF(words_per_article, all_words, float(60000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(articleTF, idfs):\n",
    "    tfidf = defaultdict(float)\n",
    "    for word, val in articleTF.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_per_article = [defaultdict(float) for i in range(60000)]\n",
    "for index, articleTF in enumerate(tf_per_article):\n",
    "    tf_idf_per_article[index] = computeTFIDF(articleTF, idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knn import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"shell_port\": 57561,\n",
      "  \"iopub_port\": 57562,\n",
      "  \"stdin_port\": 57563,\n",
      "  \"control_port\": 57564,\n",
      "  \"hb_port\": 57565,\n",
      "  \"ip\": \"127.0.0.1\",\n",
      "  \"key\": \"6cea8e45-6d02e1e5facd22da295ff9dd\",\n",
      "  \"transport\": \"tcp\",\n",
      "  \"signature_scheme\": \"hmac-sha256\",\n",
      "  \"kernel_name\": \"\"\n",
      "}\n",
      "\n",
      "Paste the above JSON into a file, and connect with:\n",
      "    $> jupyter <app> --existing <file>\n",
      "or, if you are local, you can connect with just:\n",
      "    $> jupyter <app> --existing kernel-80818862-235d-4ef5-8d8c-73b24c809089.json\n",
      "or even just:\n",
      "    $> jupyter <app> --existing\n",
      "if this is the most recent Jupyter kernel you have started.\n"
     ]
    }
   ],
   "source": [
    "%connect_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\i503708\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\dobrynin\\\\python.exe'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y):\n",
    "    return X[:59900], X[59900:], y[:59900], y[59900:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_per_article, themes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# unit tests\n",
    "knn = KNeighborsClassifier(all_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6644935607910156\n",
      "1.6735689640045166\n",
      "1.6904408931732178\n",
      "1.6247024536132812\n",
      "1.3964598178863525\n",
      "1.507927417755127\n",
      "1.8460643291473389\n",
      "1.9777147769927979\n",
      "2.006636619567871\n",
      "2.710754871368408\n",
      "1.7991905212402344\n",
      "1.5050444602966309\n",
      "1.4551122188568115\n",
      "1.6047091484069824\n",
      "2.3158226013183594\n",
      "2.6558897495269775\n",
      "1.7253875732421875\n",
      "1.2007899284362793\n",
      "1.669539451599121\n",
      "1.6834993362426758\n",
      "1.5861809253692627\n",
      "1.8300530910491943\n",
      "1.5967755317687988\n",
      "1.6884422302246094\n",
      "1.9178738594055176\n",
      "1.729377031326294\n",
      "2.0674726963043213\n",
      "1.4233348369598389\n",
      "1.6595635414123535\n",
      "1.7084791660308838\n",
      "1.2615830898284912\n",
      "1.8271148204803467\n",
      "1.8939380645751953\n",
      "1.8271598815917969\n",
      "2.113402843475342\n",
      "1.439129114151001\n",
      "2.889277696609497\n",
      "1.889948844909668\n",
      "1.7034873962402344\n",
      "1.6555759906768799\n",
      "1.7722232341766357\n",
      "1.8281128406524658\n",
      "2.7307002544403076\n",
      "1.5339021682739258\n",
      "1.7503678798675537\n",
      "1.8889408111572266\n",
      "1.9228613376617432\n",
      "1.9129064083099365\n",
      "1.7962408065795898\n",
      "1.7253944873809814\n",
      "1.451160192489624\n",
      "1.95988130569458\n",
      "1.5927410125732422\n",
      "1.6645512580871582\n",
      "1.418247938156128\n",
      "2.0444998741149902\n",
      "2.0405430793762207\n",
      "1.5698683261871338\n",
      "1.8858959674835205\n",
      "1.4173047542572021\n",
      "1.4780054092407227\n",
      "1.204982042312622\n",
      "1.7542674541473389\n",
      "1.5808184146881104\n",
      "1.722395658493042\n",
      "1.5349020957946777\n",
      "1.6665377616882324\n",
      "1.8649766445159912\n",
      "2.109361410140991\n",
      "2.079441547393799\n",
      "1.6536238193511963\n",
      "1.7722666263580322\n",
      "1.7782437801361084\n",
      "1.7692275047302246\n",
      "2.0066375732421875\n",
      "2.454439640045166\n",
      "1.3823051452636719\n",
      "2.339791774749756\n",
      "1.4879791736602783\n",
      "2.7885448932647705\n",
      "2.2888803482055664\n",
      "2.380641460418701\n",
      "2.544196367263794\n",
      "2.0286359786987305\n",
      "2.576115131378174\n",
      "2.028576612472534\n",
      "2.321795701980591\n",
      "1.8670098781585693\n",
      "1.9569849967956543\n",
      "1.3645727634429932\n",
      "1.6476030349731445\n",
      "1.744292974472046\n",
      "1.6695380210876465\n",
      "1.7453794479370117\n",
      "2.233045816421509\n",
      "1.8021252155303955\n",
      "2.0804364681243896\n",
      "2.1275179386138916\n",
      "2.8972558975219727\n",
      "1.6855409145355225\n"
     ]
    }
   ],
   "source": [
    "prediction = knn.test(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sport',\n",
       " 'science',\n",
       " 'economics',\n",
       " 'media',\n",
       " 'media',\n",
       " 'culture',\n",
       " 'economics',\n",
       " 'economics',\n",
       " 'life',\n",
       " 'life']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_by_freq = dict(list(sorted(all_words.items(), key=lambda x: x[1], reverse=True))[:words_number_leave])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = dict((k,i) for i,k in enumerate(sort_by_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = [[[0]*len(sort_by_freq)] for _ in range(number_of_rows)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"news_train.txt\", \"r\", encoding=\"utf-8\") as train_text:\n",
    "    for ith, article in enumerate(train_text):\n",
    "        split = article.split('\\t')\n",
    "        text = (split[1] + \" \" + split[2]).lower()\n",
    "        words = regex_string.split(text)\n",
    "        for word in words:\n",
    "            if word in sort_by_freq:\n",
    "                count_matrix[ith][word_index[word]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix[:][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{k: all_words[k] for v, k in list())[:100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes_enc = dict((k,i) for i,k in enumerate(set(themes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes_transformed = [themes_enc[theme] for theme in themes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes_transformed[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = dict((k,i) for i,k in enumerate(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_common_item(array):\n",
    "    count_dict = defaultdict(int)\n",
    "    for key in array:\n",
    "        count_dict[key] += 1\n",
    "    key, count = max(count_dict.items(), key=itemgetter(1))\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(A,B): \n",
    "    return (sum(a*b for a,b in zip(A,B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a,b):\n",
    "    return dot(a,b) / ( (dot(a,a) **.5) * (dot(b,b) ** .5) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(words_train, words_test): \n",
    "    #return len(words_train) + len(words_test) - 2 * len(words_train & words_test)\n",
    "    train_row = [0] * len(columns)\n",
    "    print(\"RAZ\")\n",
    "    for word_train in words_train:\n",
    "        train_row[columns[word_train]] = 1\n",
    "    print(\"DVA\")\n",
    "    test_row = [0] * len(columns)\n",
    "    for word_test in words_test:\n",
    "        test_row[columns[word_test]] = 1\n",
    "    print(\"TRI!\")\n",
    "    return cosine_similarity(train_row, test_row)#math.sqrt(sum([(train_row[i]-test_row[i])**2 for i, _ in enumerate(train_row)]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(k=1):\n",
    "    y_test = []\n",
    "    with open(\"news_test.txt\", \"r\", encoding=\"utf-8\") as test_text:\n",
    "        for test_article in test_text:\n",
    "            split = article.split('\\t')\n",
    "            text = split[1] + \" \" + split[2]\n",
    "            test_words = set(re.split(\"(\\w[\\w']*\\w|\\w)\", text))\n",
    "            test_words &= all_words\n",
    "            eucl_dist = [euclidean_dist(train_words, test_words) for train_words in words_per_article]\n",
    "            sorted_eucl_dist = sorted(eucl_dist)\n",
    "            closest_knn = [eucl_dist.index(sorted_eucl_dist[i]) for i in range(0, k)] if k > 1 else [eucl_dist.index(min(eucl_dist))]\n",
    "            closest_labels_knn = [themes_transformed[x] for x in closest_knn]\n",
    "            y_test.append(get_most_common_item(closest_labels_knn))\n",
    "            print(get_most_common_item(closest_labels_knn))\n",
    "        \n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn(k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dobrynin",
   "language": "python",
   "name": "dobrynin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
